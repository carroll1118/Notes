@[toc]
## 1. 特征工程有哪些？
> 特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数 据的过程。在实际工作中，**特征工程旨在去除原始数据中的杂质和冗余**，设计更高效的特征以刻画求解的问题与预测模型之间的关系。

**主要讨论以下两种常用的数据类型**：

* **结构化数据**：结构化数据类型可以看作关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型；每一行数据表示一个样本 的信息。
* **非结构化数据**：非结构化数据主要包括文本、图像、音频、视频数据， 其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数据的大小各不相同。
### 1.1 数据处理
#### 异常值处理
> 异常值分析是检验数据是否有录入错误以及含有不合常理的数据；

* 异常值是指样本中的个别值,其数据明显偏离其余的观测值。异常值也称为离群点,异常值的分析也称为离群点分析。

**异常值处理一般分为以下几个步骤：异常值检测、异常值筛选、异常值处理。**
* **异常值检测**的方法主要有：箱型图、简单统计量（比如观察极(大／小)值），3σ原则 ；
* **异常值处理**方法主要有：删除法、插补法、替换法。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200909002246778.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70#pic_center)

####  缺失值处理
> 由于我们的特征矩阵由两种类型的数据组成：分类型和连续型，因此我们必须对两种数据采用不同的填补缺失值策略。
> 传统地，如果是分类型特征，我们则采用众数进行填补。如果是连续型特征，我们则采用均值来填补

**关于缺失值处理的方式， 有几种情况**：
* 不处理（这是针对xgboost等树模型），有些模型有处理缺失的机制，所以可以不处理
* 如果缺失的太多，可以考虑删除该列
* 插值补全（均值，中位数，众数，建模预测，多重插补等）
* 分箱处理，缺失值一个箱。

**下面整理几种填充值的方式：**
```python
# 删除重复值
data.drop_duplicates()
# dropna()可以直接删除缺失样本，但是有点不太好

# 填充固定值
train_data.fillna(0, inplace=True) # 填充 0
data.fillna({0:1000, 1:100, 2:0, 4:5})   # 可以使用字典的形式为不用列设定不同的填充值

train_data.fillna(train_data.mean(),inplace=True) # 填充均值
train_data.fillna(train_data.median(),inplace=True) # 填充中位数
train_data.fillna(train_data.mode(),inplace=True) # 填充众数

train_data.fillna(method='pad', inplace=True) # 填充前一条数据的值，但是前一条也不一定有值
train_data.fillna(method='bfill', inplace=True) # 填充后一条数据的值，但是后一条也不一定有值

"""插值法：用插值法拟合出缺失的数据，然后进行填充。"""
for f in features: 
    train_data[f] = train_data[f].interpolate()

train_data.dropna(inplace=True)

"""填充KNN数据：先利用knn计算临近的k个数据，然后填充他们的均值"""
from fancyimpute import KNN
train_data_x = pd.DataFrame(KNN(k=6).fit_transform(train_data_x), columns=features)
```
### 1.2 特征归一化
* 为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。例如，分析一个人的身高和体重对健康的影响，如果 使用米（m）和千克（kg）作为单位，那么身高特征会在1.6～1.8m的数值范围内，体重特征会在50～100kg的范围内，分析出来的**结果显然会倾向于数值差别比较大的体重特征**。想要得到更为准确的结果，就需要进行**特征归一化** （Normalization）处理，使各指标处于同一数值量级，以便进行分析。

**对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内。最常用的方法主要有以下两种：**

#### 线性函数归一化（Min-Max Scaling）
* 它对原始数据进行线性变换，使 结果映射到[0, 1]的范围，实现对原始数据的等比缩放。归一化公式如下，其中X为原始数据，Xmax,Xmin 分别为数据最大值和最小值。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226140027215.png)
####  零均值归一化（Z-Score Normalization）
* 它会将原始数据映射到均值为 0、标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为σ，那么归一化公式定义为
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226140036324.png)
优点：**训练数据归一化后，容易更快地通过梯度下降找到最优解。**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226140305113.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)

> 注意：数据归一化并不是万能的。在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。但对于**决策树模型则并不适用**。

### 1.3 类别型特征

> 类别型特征（Categorical Feature）主要是指性别（男、女）、血型（A、B、 AB、O）等只在**有限选项内取值**的特征。类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。
#### 序号编码
* **序号编码通常用于处理类别间具有大小关系的数据**。例如成绩，可以分为 低、中、高三档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值ID，例如高表示为3、中表示为2、低表示为1，转换后依然保留了大小关系。

#### 独热编码(one-hot)
* **独热编码通常用于处理类别间不具有大小关系的特征**。例如血型，一共有4个 取值（A型血、B型血、AB型血、O型血），独热编码会把血型变成一个4维稀疏向量，A型血表示为（1, 0, 0, 0），B型血表示为（0, 1, 0, 0），AB型表示为（0, 0, 1, 0），O型血表示为（0, 0, 0, 1）。对于类别取值较多的情况下使用独热编码。
#### 二进制编码 
* 二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后 将类别ID对应的二进制编码作为结果。以A、B、AB、O血型为例，下图是二进制编码的过程。A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为 010；以此类推可以得到AB型血和O型血的二进制表示。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226140616766.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)
### 1.4 高维组合特征的处理
> 为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

* 以广告点击预估问题为例，原始数据有语言和类型两种离散特征，第一张图是语言和类型对点击的影响。为了提高拟合能力，语言和类型可以组成二阶特征，第二张图是语言和类型的组合特征对点击的影响。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226140627419.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)
### 1.5 文本表示模型

> 文本是一类非常重要的非结构化数据，如何表示文本数据一直是机器学习领域的一个重要研究方向。

#### 词袋模型和N-gram模型
* 最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开， 然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用**TF-IDF**来计算权重。

> **TF-IDF**（term frequency–inverse document frequency，词频-逆向文件频率）是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用加权技术。
**TF-IDF**是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
 **TF-IDF**的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

#### 主题模型
* 主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。
#### 词嵌入与深度学习模型
* 词嵌入是一类将词向量化的模型的统称，**核心思想**是将每个词都映射成低维空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。K维空间的每一 维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。


### 1.6 数据分桶
**连续值经常离散化或者分离成“箱子”进行分析, 为什么要做数据分桶呢？**
* 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
* 离散后的特征对异常值更具**鲁棒性**，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰； 
* LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；
* 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
* 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化

> 鲁棒性：就是不受异常值影响，一般是鲁棒性高的数据，比较优质。

当然还有很多原因，LightGBM 在改进 XGBoost 时就增加了数据分桶，增强了模型的泛化性。数据分桶的方式：
* 等频分桶
* 等距分桶
* Best-KS分桶（类似利用基尼指数进行二分类）
* 卡方分桶

最好将数据分桶的特征作为新一列的特征，不要把原来的数据给替换掉。

### 1.7 特征构造
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200831163036284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70#pic_center)
在特征构造的时候，需要借助一些背景知识，遵循的一般原则就是需要发挥想象力，尽可能多的创造特征，不用先考虑哪些特征可能好，可能不好，先弥补这个广度。特征构造的时候需要考虑数值特征，类别特征，时间特征。
* 对于数值特征，一般会尝试一些它们之间的加减组合（当然不要乱来，根据特征表达的含义）或者提取一些统计特征
* 对于类别特征，我们一般会尝试之间的交叉组合，embedding也是一种思路
* 对于时间特征，这一块又可以作为一个大专题来学习，在时间序列的预测中这一块非常重要，也会非常复杂，需要就尽可能多的挖掘时间信息，会有不同的方式技巧。当然在这个比赛中涉及的实际序列数据有一点点，不会那么复杂。

### 1.8 特征选择
**特征选择主要有两个功能**：
* 减少特征数量、降维，使模型泛化能力更强，减少过拟合
* 增强对特征和特征值之间的理解

**通常来说，从两个方面考虑来选择特征**：
* 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
* 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。

**根据特征选择的形式又可以将特征选择方法分为3种**：
* Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
* Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
* Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

#### 过滤式
* **主要思想**: 对每一维特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。相当于先对特征进行过滤操作，然后用特征子集来训练分类器。
* **主要方法**：
	* **移除低方差的特征**：比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以**无论接下来的特征工程要做什么，都要优先消除方差为0的特征。**
	* **相关系数排序**：分别计算每个特征与输出值之间的相关系数，设定一个阈值，选择相关系数大于阈值的部分特征；我们有三种常用的方法来评判特征与标签之间的相关性：卡方，F检验，互信息。
		* 利用假设检验得到特征与输出值之间的相关性，方法有比如卡方检验、t检验、F检验等。
		* 互信息，利用互信息从信息熵的角度分析相关性。
		* 卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。
* 卡方检验的本质是推测两组数据之间的差异，其检验的原假设是”两组数据是相互独立的”。卡方检验返回卡方值和P值两个统计量，其中卡方值很难界定有效的范围，而p值，我们一般使用0.01或0.05作为显著性水平，即p值判断的边界，具体如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200831164738897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70#pic_center)
* 从特征工程的角度，我们希望选取卡方值很大，p值小于0.05的特征，即和标签是相关联的特征。


> 为什么随机森林运行如此之快？为什么方差过滤对随机森林没很大的有影响？
* 这是由于两种算法的原理中涉及到的计算量不同。
* 最近邻算法KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要**遍历特征或升维**来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。
* 但对于不需要遍历特征的算法，比如随机森林，它**随机选取特征进行分枝**，本身运算就非常快速，因此特征选择对它来说效果平平。这其实很容易理解，无论过滤法如何降低特征的数量，随机森林也只会选取固定数量的特征来建模；而最近邻算法就不同了，特征越少，距离计算的维度就越少，模型明显会随着特征的减少变得轻量。因此，过滤法的主要对象是：需要遍历特征或升维的算法们，而过滤法的主要目的是：在维持算法表现的前提下，帮助算法们降低计算成本。

**提供一些有价值的小tricks**：
* 对于数值型特征，方差很小的特征可以不要，因为太小没有什么区分度，提供不了太多的信息，对于分类特征，也是同理，取值个数高度偏斜的那种可以先去掉。
* 根据与目标的相关性等选出比较相关的特征（当然有时候根据字段含义也可以选）
* 卡方检验一般是检查离散变量与离散变量的相关性，当然离散变量的相关性信息增益和信息增益比也是不错的选择（可以通过决策树模型来评估来看），person系数一般是查看连续变量与连续变量的线性相关关系。

#### 包裹式
* 单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。
* **主要思想**：包裹式从初始特征集合中不断的选择特征子集，训练学习器，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。包裹式特征选择直接针对给定学习器进行优化。
* **主要方法**：递归特征消除算法, 基于机器学习模型的特征排序
* **优缺点**：
	* 优点：从最终学习器的性能来看，包裹式比过滤式更好；
	* 缺点：由于特征选择过程中需要多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择要大得多。

#### 嵌入式
* 在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器 训练过程中自动地进行特征选择。嵌入式选择最常用的是L1正则化与L2正则化。在对线性回归模型加入两种正则化方法后，他们分别变成了岭回归与Lasso回归。
* **主要思想**：在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。
* **主要方法**：简单易学的机器学习算法–岭回归（Ridge Regression），就是线性回归过程加入了L2正则项。

* L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择
* L2正则化在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参 数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性 回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移 得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。

#### PCA降维技术
* 通过上面的特征选择部分，可以选出更好的分析特征，但是如果这些特征维度仍然很高怎么办？
* 如果数据特征维度太高，首先计算很麻烦，其次增加了问题的复杂程度，分析起来也不方便。这时候就会想是不是再去掉一些特征就好了呢？但是这个特征也不是凭自己的意愿去掉的，因为盲目减少数据的特征会损失掉数据包含的关键信息，容易产生错误的结论，对分析不利。
* 所以想找到一个合理的方式，既可以减少需要分析的指标，而且尽可能多的保持原来数据的信息，PCA就是这个合理的方式之一。 但要注意一点， 特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，而PCA，将已存在的特征压缩，降维完毕后不是原来特征的任何一个，也就是PCA降维之后的特征我们根本不知道什么含义了。

> PCA一般不适用于探索特征和标签之间的关系的模型（如线性回归），因为无法解释的新特征和标签之间的关系不具有意义。在线性回归模型中，我们使用特征选择。

### 1.9 特征工程脑图
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226140904485.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)
## 2. 机器学习优化方法

> 优化是应用数学的一个分支，也是机器学习的核心组成部分。实际上，机器学习算法 = 模型表征 + 模型评估 + 优化算法。其中，优化算法所做的事情就是在 模型表征空间中找到模型评估指标最好的模型。不同的优化算法对应的模型表征 和评估指标不尽相同。
### 2.1 常用损失函数
> 损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的准确性就越好。常见的损失函数如下：

#### 平方损失函数
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226141104465.png)
* Y-f(X)表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和。而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022614111469.png)
* **该损失函数一般使用在线性回归当中**。

#### 0-1损失函数(zero-one loss)
* 0-1损失是指预测值和目标值不相等为1， 否则为0:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200909002658690.png#pic_center)

**特点**
* 0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.
* (2)**感知机就是用的这种损失函数**。但是相等这个条件太过严格，因此可以放宽条件，即满足 **|Y - f(x)|  < T**  时认为相等，
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200909003004608.png#pic_center)
#### 绝对值损失函数
* 绝对值损失函数是计算预测值与目标值的差的绝对值：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200909003054772.png#pic_center)

#### log损失函数
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226141144242.png)
* 公式中的 y=1 表示的是真实值为1时用第一个公式，真实 y=0 用第二个公式计算损失。为什么要加上log函数呢？可以试想一下，当真实样本为1是，但h=0概率，那么log0=∞，这就对模型最大的惩罚力度；当h=1时，那么log1=0，相当于没有惩罚，也就是没有损失，达到最优结果。所以数学家就想出了用log函数来表示损失函数。

* 最后按照梯度下降法一样，求解极小值点，得到想要的模型效果。**该损失函数一般使用在逻辑回归中**。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226141157773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)

#### Hinge损失函数
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226142822453.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)

SVM采用的就是Hinge Loss，用于“最大间隔(max-margin)”分类。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226141408362.png)
####  交叉熵损失函数 (Cross-entropy loss function)
* 交叉熵损失函数的标准形式如下:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200909003341202.png#pic_center)
* 公式中 x 表示样本， y 表示实际的标签， a 表示预测的输出，n  表示样本总数量。

**特点**
* 本质上也是一种对数似然函数，可用于二分类和多分类任务中。
	* 二分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200909003506389.png#pic_center)
	* 多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200909003515432.png#pic_center)
* 当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

> 对数损失函数和交叉熵损失函数应该是等价的。

### 2.2 什么是凸优化

* 凸函数的严格定义为，函数L(·) 是凸函数当且仅当对定义域中的任意两点x，y和任意实数λ∈[0,1]总有：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226141439764.png)

* 该不等式的一个直观解释是，凸函数曲面上任意两点连接而成的线段，其上的任意一点都不会处于该函数曲面的下方，如下图所示所示。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226141456867.png)
* 凸优化问题的例子包括支持向量机、线性回归等线性模型，非凸优化问题的例子包括低秩模型（如矩阵分解）、深度神经网络模型等。

### 2.3 正则化项
> L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓“惩罚”是指对损失函数中的某些参数做一些限制。

* 对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200909004356627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70#pic_center)
**L1正则化和L2正则化的说明如下**：
* L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为`∣∣w∣∣1`
* L2正则化是指权值向量w中各个元素的平方和然后再求平方根，通常表示为`∣∣w∣∣2`

==一般都会在正则化项之前添加一个系数，用α表示或用λ表示。这个系数需要用户指定。==

使用正则化项，也就是给loss function加上一个参数项，正则化项有**L1正则化、L2正则化**。加入这个正则化项好处：

* 控制参数幅度，不让模型“无法无天”。
* 限制参数搜索空间
* 解决欠拟合与过拟合的问题。
* L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
* L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

### 2.4 常见的几种最优化方法
#### 梯度下降法
* 梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。
* 一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。
* 梯度下降法的搜索迭代示意图如下图所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226141827456.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)

缺点：靠近极小值时收敛速度减慢；直线搜索时可能会产生一些问题；可能会“之字形”地下降。

#### 牛顿法

牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。具体步骤：

* 首先，选择一个接近函数 f (x)零点的 x0，计算相应的 f (x0) 和切线斜率f ' (x0)（这里f ' 表示函数 f 的导数）。

* 然后我们计算穿过点(x0, f (x0)) 并且斜率为f '(x0)的直线和 x 轴的交点的x坐标，也就是求如下方程的解：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226141838689.png)
* 我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f (x) = 0的解。因此我们现在可以利用x1开始下一轮迭代。

由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法搜索动态示例图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226141952424.gif)

从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。缺点：

* 牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
* 在高维情况下这个矩阵非常大，计算和存储都是问题。
* 在小批量的情况下，牛顿法对于二阶导数的估计噪声太大。
* 目标函数非凸的时候，牛顿法容易受到鞍点或者最大值点的吸引。
#### 拟牛顿法

* 拟牛顿法是求解非线性优化问题最有效的方法之一，**本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度**。
* 拟牛顿法和梯度下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于梯度下降法，尤其对于困难的问题。
* 另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

#### 共轭梯度法

* 共轭梯度法是介于梯度下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 
* 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。

下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226142046316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)
### 2.5 降维方法
#### 线性判别分析（LDA）
* **线性判别分析**（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和**主成分分析PCA**不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。

LDA分类思想简单总结如下：

* 多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。
* 对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。
* 对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。

如果用一句话概括LDA思想，**即“投影后类内方差最小，类间方差最大”。**

假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226142126198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)
左图和右图是两种不同的投影方式。

* 左图思路：让不同类别的平均点距离最远的投影方式。
* 右图思路：让同类别的数据挨得最近的投影方式。

​ 从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。

​ 以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。

   **优点**
* 可以使用类别的先验知识；
* 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；

**缺点**
* LDA不适合对非高斯分布样本进行降维；
* LDA降维最多降到分类数k-1维；
* LDA在样本分类信息依赖方差而不是均值时，降维效果不好；
* LDA可能过度拟合数据。
#### 主成分分析（PCA）
* PCA就是将高维的数据通过线性变换投影到低维空间上去。
* 投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。
* 去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。
* 去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。
* 对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。
* 完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。
* 之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。

**图解PCA**
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200909005243543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70#pic_center)

* PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n < m），这n维形成主元，是重构出来最能代表原始数据的正交特征。

* ​ 假设数据集是m个n维，$(\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \cdots, \boldsymbol x^{(m)})$。如果$n=2$，需要降维到$n'=1$，现在想找到某一维度方向代表这两个维度的数据。

**有以下两个主要评价指标：**

* 样本点到这个直线的距离足够近。
* 样本点在这个直线上的投影能尽可能的分开。

如果我们需要降维的目标维数是其他任意维，则：

* 样本点到这个超平面的距离足够近。
* 样本点在这个超平面上的投影能尽可能的分开。

**优点**	
* 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　
* 各主成分之间正交，可消除原始数据成分间的相互影响的因素。
* 计算方法简单，主要运算是特征值分解，易于实现。

**缺点**
* 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。
* 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。
#### 比较这两种方法
**降维的必要性：**

1. 多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。
2. 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。
3. 过多的变量，对查找规律造成冗余麻烦。
4. 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。

**降维的目的：**


* 减少预测变量的个数。
* 确保这些变量是相互独立的。
* 提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。
* 数据在低维下更容易处理、更容易使用。
* 去除数据噪声。
* 降低算法运算开销。

**LDA和PCA区别**
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226142647669.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)
## 3. 机器学习评估方法

> 混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。具体评价指标有总体精度、制图精度、用户精度等，这些精度指标从不同的侧面反映了图像分类的精度。

###  3.1 性能度量
#### 准确率(Accuracy)
**准确率（Accuracy）**。顾名思义，就是所有的预测正确（正类负类）的占总的比重。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226143611860.png)
* 准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。所以，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准 确率的最主要因素。

#### 精确率（Precision）
* 精确率（Precision），查准率。即正确预测为正的占全部预测为正的比例。个人理解：真正正确的占所有预测为正的比例。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226143620426.png)

#### 召回率(Recall)
* 召回率（Recall），查全率。即正确预测为正的占全部实际为正的比例。个人理解：真正正确的占所有实际为正的比例。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226143627410.png)

* 为了综合评估一个排序模型的好坏，不仅要看模型在不同 Top N下的Precision@N和Recall@N，而且最好绘制出模型的P-R（Precision- Recall）曲线。这里简单介绍一下P-R曲线的绘制方法。

* P-R曲线的横轴是召回率，纵轴是精确率。对于一个排序模型来说，其P-R曲 线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本， 小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R 曲线是通过将阈值从高到低移动而生成的。下图是P-R曲线样例图，其中实线代表 模型A的P-R曲线，虚线代表模型B的P-R曲线。原点附近代表当阈值最大时模型的 精确率和召回率。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226143634679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)

* 由图可见，当召回率接近于0时，模型A的精确率为0.9，模型B的精确率是1， 这说明模型B得分前几位的样本全部是真正的正样本，而模型A即使得分最高的几 个样本也存在预测错误的情况。并且，随着召回率的增加，精确率整体呈下降趋 势。但是，当召回率为1时，模型A的精确率反而超过了模型B。这充分说明，只用某个点对应的精确率和召回率是不能全面地衡量模型的性能，只有通过P-R曲线的 整体表现，才能够对模型进行更为全面的评估。

#### F1值(H-mean值)
* F1值（H-mean值）。F1值为算数平均数除以几何平均数，且越大越好，将Precision和Recall的上述公式带入会发现，当F1值小时，True Positive相对增加，而false相对减少，即Precision和Recall都相对增加，即F1对Precision和Recall都进行了加权。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226143648564.png)![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226143653600.png)

#### ROC曲线
* ROC曲线。接收者操作特征曲线（receiver operating characteristic curve），是反映敏感性和特异性连续变量的综合指标，ROC曲线上每个点反映着对同一信号刺激的感受性。
* 下图是ROC曲线例子。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226143733331.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70)

* 横坐标：1-Specificity，伪正类率(False positive rate，FPR，FPR=FP/(FP+TN))，预测为正但实际为负的样本占所有负例样本的比例；

* 纵坐标：Sensitivity，真正类率(True positive rate，TPR，TPR=TP/(TP+FN))，预测为正且实际为正的样本占所有正例样本的比例。

* 真正的理想情况，TPR应接近1，FPR接近0，即图中的（0,1）点。ROC曲线越靠拢（0,1）点，越偏离45度对角线越好。

**AUC值**

* AUC (Area Under Curve) 被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。

**从AUC判断分类器（预测模型）优劣的标准**：

* AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
* 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
* AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
* AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

**一句话来说，AUC值越大的分类器，正确率越高。**

#### 余弦距离和欧式距离
余弦距离： ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226143824100.png)


**欧式距离**:在数学中，欧几里得距离或欧几里得度量是欧几里得空间中两点间“普通”（即直线）距离。

* 对于两个向量A和B，余弦距离关注的是向量之间的角度关系，并不关心它们的绝对大小，其取值 范围是[−1,1]。当一对文本相似度的长度差距很大、但内容相近时，如果使用词频 或词向量作为特征，它们在特征空间中的的欧氏距离通常很大；而如果使用余弦 相似度的话，它们之间的夹角可能很小，因而相似度高。
* 此外，在文本、图像、 视频等领域，研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保 持“相同时为1，正交时为0，相反时为−1”的性质，而欧氏距离的数值则受维度的 影响，范围不固定，并且含义也比较模糊。

#### A/B测试
* AB测试是为Web或App界面或流程制作两个（A/B）或多个（A/B/n）版本，在同一时间维度，分别让组成成分相同（相似）的访客群组（目标人群）随机的访问这些版本，收集各群组的用户体验数据和业务数据，最后分析、评估出最好版本，正式采用。

### 3.2 模型评估方法
#### Holdout检验

* Holdout 检验是最简单也是最直接的验证方法，它将原始的样本集合随机划分 成训练集和验证集两部分。比方说，对于一个点击率预测模型，我们把样本按照 70%～30% 的比例分成两部分，70% 的样本用于模型训练；30% 的样本用于模型 验证，包括绘制ROC曲线、计算精确率和召回率等指标来评估模型性能。

* Holdout 检验的缺点很明显，即在验证集上计算出来的最后评估指标与原始分 组有很大关系。为了消除随机性，研究者们引入了“交叉检验”的思想。

#### 交叉检验
* 交叉验证是用来观察模型的稳定性的一种方法，我们将数据划分为n份，依次使用其中一份作为测试集，其他n-1份
作为训练集，多次计算模型的精确性来评估模型的平均准确程度。训练集和测试集的划分会干扰模型的结果，因此
用交叉验证n次的结果求出的平均值，是对模型效果的一个更好的度量。在实际实验中，n经常取10。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200831152647949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzIyODI3,size_16,color_FFFFFF,t_70#pic_center)

#### 自助法
* 不管是Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行 模型评估的。然而，当样本规模比较小时，将样本集进行划分会让训练集进一步 减小，这可能会影响模型训练效果。有没有能维持训练集样本规模的验证方法 呢？自助法可以比较好地解决这个问题。

* 自助法是基于自助采样法的检验方法。对于总数为n的样本集合，进行n次有 放回的随机抽样，得到大小为n的训练集。n次采样过程中，有的样本会被重复采 样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验 证，这就是自助法的验证过程。

### 3.3 超参数调优

> 为了进行超参数调优，我们一般会采用网格搜索、随机搜索、贝叶斯优化等 算法。在具体介绍算法之前，需要明确超参数搜索算法一般包括哪几个要素。一 是目标函数，即算法需要最大化/最小化的目标；二是搜索范围，一般通过上限和 下限来确定；三是算法的其他参数，如搜索步长。
#### 网格搜索
* **网格搜索**，可能是最简单、应用最广泛的超参数搜索算法，它通过查找搜索范 围内的所有的点来确定最优值。如果采用较大的搜索范围以及较小的步长，网格 搜索有很大概率找到全局最优值。然而，这种搜索方案十分消耗计算资源和时间，特别是需要调优的超参数比较多的时候。因此，在实际应用中，网格搜索法一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然 后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。这种操作方案可以降低 所需的时间和计算量，但由于目标函数一般是非凸的，所以很可能会错过全局最 优值。
#### 随机搜索
* **随机搜索**，随机搜索的思想与网格搜索比较相似，只是不再测试上界和下界之间的所有 值，而是在搜索范围中随机选取样本点。它的理论依据是，如果样本点集足够 大，那么通过随机采样也能大概率地找到全局最优值，或其近似值。随机搜索一 般会比网格搜索要快一些，但是和网格搜索的快速版一样，它的结果也是没法保证的。
#### 贝叶斯优化算法
* **贝叶斯优化算法**，贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全 不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息； 而贝叶斯优化算法则充分利用了之前的信息。贝叶斯优化算法通过对目标函数形 状进行学习，找到使目标函数向全局最优值提升的参数。
### 3.4 过拟合和欠拟合
* 过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是 模型在训练集上的表现很好，但在测试集和新数据上的表现较差。欠拟合指的是 模型在训练和预测时表现都不好的情况。下图形象地描述了过拟合和欠拟合的区别。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226144000570.png)

#### 防止过拟合：
* 从数据入手，获得更多的训练数据。
* 降低模型复杂度。
* 正则化方法，给模型的参数加上一定的正则约束。
* 集成学习方法，集成学习是把多个模型集成在一起。
#### 防止欠拟合：
* 添加新特征。
* 增加模型复杂度。
* 减小正则化系数。
## 4. 检验方法
> 随着学习算法种类，特征转换方式，正则化方式等等的增加，在不同的组合之下我们就会得到种类非常多的学习模型。而在实务上我们通常想要的就是那个Eout（`Eout：未知的目标函数 f 在训练样本内的误差 和 在训练样本之外的误差`）最小的模型，所以我们在面临众多的学习模型的时候需要作出选择，而模型检验结果的好坏正是我们作出选择的依据。
### 4.1  KS检验
Kolmogorov-Smirnov检验是基于累计分布函数的，用于检验一个分布是否符合某种理论分布或比较两个经验分布是否有显著差异。

* 单样本K-S检验是用来检验一个数据的观测经验分布是否符合已知的理论分布。
* 两样本K-S检验由于对两样本的经验分布函数的位置和形状参数的差异都敏感，所以成为比较两样本的最有用且最常用的非参数方法之一。
检验统计量为：![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226144041309.png)

其中 Fn(x)为观察序列值，F(x)为理论序列值或另一观察序列值。

### 4.2  T检验
* T检验，也称student t检验，主要用户样本含量较小，总体标准差未知的正态分布。

* t检验是用t分布理论来推论差异发生的概率，从而比较两个平均数的差异是否显著。

* t检验分为单总体检验和双总体检验。

### 4.3  F检验
* T检验和F检验的由来：为了确定从样本中的统计结果推论到总体时所犯错的概率。F检验又叫做联合假设检验，也称方差比率检验、方差齐性检验。是由英国统计学家Fisher提出。通过比较两组数据的方差，以确定他们的精密度是否有显著性差异。

### 4.4  Grubbs检验
* 一组测量数据中，如果个别数据偏离平均值很远，那么称这个数据为“可疑值”。用格拉布斯法判断，能将“可疑值”从测量数据中剔除。

### 4.5  卡方检验
卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。

* 提出原假设H0：总体X的分布函数F(x)；

* 将总体x的取值范围分成k个互不相交的小区间A1-Ak；

* 把落入第i个区间Ai的样本的个数记做fi，成为组频数，f1+f2+f3+...+fk = n；

* 当H0为真时，根据假设的总体理论分布，可算出总体X的值落入第i个小区间Ai的概率pi，于是n*pi就是落入第i个小区间Ai的样本值的理论频数；

* 当H0为真时，n次试验中样本落入第i个小区间Ai的频率fi/n与概率pi应该很接近。基于这种思想，皮尔逊引入检测统计量：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200226144148562.png)
在H0假设成立的情况下服从自由度为k-1的卡方分布。

**KS检验与卡方检验**

**相同点**:都采用实际频数和期望频数只差进行检验

**不同点**：

* 卡方检验主要用于类别数据，而KS检验主要用于有计量单位的连续和定量数据。
* 卡方检验也可以用于定量数据，但必须先将数据分组才能获得实际的观测频数，而KS检验能直接对原始数据进行检验，所以它对数据的利用比较完整。

**你知道的越多，你不知道的越多。
有道无术，术尚可求，有术无道，止于术。
如有其它问题，欢迎大家留言，我们一起讨论，一起学习，一起进步**
